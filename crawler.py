#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·ä¸“è¾‘æ–‡ç« æŠ“å–è„šæœ¬ä¸»ç¨‹åº
"""

import os
import sys
import json
import time
import argparse
import logging
from datetime import datetime
from urllib.parse import urljoin, urlparse

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

from config import (BASE_DIR, ARTICLES_DIR, LOGS_DIR, JSON_FILE,
                   DEFAULT_DELAY, get_random_delay, SELECTORS, SCROLL_PAUSE_TIME,
                   HEADLESS, WINDOW_SIZE)
from utils import (setup_driver, setup_logging, load_json_state, save_json_state,
                   validate_url, get_article_status, update_article_status,
                   save_article_content, scroll_to_bottom, clean_filename,
                   extract_title_from_preview, format_progress_bar, extract_url_hash,
                   check_article_exists_by_url, smart_save_article_content,
                   extract_real_title_from_content, update_articles_with_url_matching,
                   extract_publish_time_from_article, parse_wechat_time_text,
                   load_date_counter, save_date_counter)

class WeChatAlbumCrawler:
    """å¾®ä¿¡å…¬ä¼—å·ä¸“è¾‘æ–‡ç« æŠ“å–å™¨"""

    def __init__(self, headless=False, delay=DEFAULT_DELAY):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.headless = headless
        self.delay = delay
        self.driver = None
        self.articles_data = None

        # è®¾ç½®æ—¥å¿—
        setup_logging(
            log_level='INFO',
            log_file=os.path.join(LOGS_DIR, "crawler.log"),
            error_log_file=os.path.join(LOGS_DIR, "errors.log")
        )

        logging.info("å¾®ä¿¡å…¬ä¼—å·ä¸“è¾‘æ–‡ç« æŠ“å–å™¨åˆå§‹åŒ–å®Œæˆ")

    def setup_driver(self):
        """è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
        try:
            self.driver = setup_driver(headless=self.headless, window_size=WINDOW_SIZE)
            logging.info("æµè§ˆå™¨é©±åŠ¨è®¾ç½®æˆåŠŸ")
            return True
        except Exception as e:
            logging.error(f"è®¾ç½®æµè§ˆå™¨é©±åŠ¨å¤±è´¥: {e}")
            return False

    def load_album_page(self, album_url):
        """åŠ è½½ä¸“è¾‘é¡µé¢"""
        try:
            if not validate_url(album_url):
                logging.error(f"æ— æ•ˆçš„URL: {album_url}")
                return False

            logging.info(f"å¼€å§‹åŠ è½½ä¸“è¾‘é¡µé¢: {album_url}")
            self.driver.get(album_url)

            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŠ è½½
            if "mp.weixin.qq.com" not in self.driver.current_url:
                logging.error("é¡µé¢åŠ è½½å¤±è´¥ï¼Œå¯èƒ½è¢«é‡å®šå‘")
                return False

            logging.info("ä¸“è¾‘é¡µé¢åŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            logging.error(f"åŠ è½½ä¸“è¾‘é¡µé¢å¤±è´¥: {e}")
            return False

    def extract_album_info(self):
        """æå–ä¸“è¾‘åŸºæœ¬ä¿¡æ¯"""
        try:
            # æå–ä¸“è¾‘æ ‡é¢˜
            title_element = self.driver.find_element(By.CSS_SELECTOR, "#js_tag_name")
            album_title = title_element.text.strip() if title_element else "æœªçŸ¥ä¸“è¾‘"

            # æå–æ–‡ç« æ€»æ•°
            desc_element = self.driver.find_element(By.CSS_SELECTOR, "#js_desc_area span")
            total_articles = 0
            if desc_element:
                desc_text = desc_element.text
                # æå–æ•°å­—ï¼Œå¦‚ "263ä¸ªå†…å®¹"
                import re
                match = re.search(r'(\d+)', desc_text)
                if match:
                    total_articles = int(match.group(1))

            return album_title, total_articles

        except Exception as e:
            logging.error(f"æå–ä¸“è¾‘ä¿¡æ¯å¤±è´¥: {e}")
            return "æœªçŸ¥ä¸“è¾‘", 0

    def load_all_articles(self):
        """åŠ è½½æ‰€æœ‰æ–‡ç« åˆ—è¡¨"""
        try:
            logging.info("å¼€å§‹åŠ è½½æ‰€æœ‰æ–‡ç« ...")

            last_count = 0
            no_change_count = 0
            max_no_change = 3  # è¿ç»­3æ¬¡æ²¡æœ‰å˜åŒ–å°±åœæ­¢

            while no_change_count < max_no_change:
                # æ»šåŠ¨åˆ°åº•éƒ¨
                scroll_to_bottom(self.driver, SCROLL_PAUSE_TIME)

                # è·å–å½“å‰æ–‡ç« æ•°é‡
                articles_container = self.driver.find_element(By.CSS_SELECTOR, SELECTORS['album_container'])
                current_articles = articles_container.find_elements(By.CSS_SELECTOR, SELECTORS['album_items'])
                current_count = len(current_articles)

                logging.info(f"å½“å‰å·²åŠ è½½ {current_count} ç¯‡æ–‡ç« ")

                # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
                if current_count == last_count:
                    no_change_count += 1
                    logging.info(f"æ–‡ç« æ•°é‡æ— å˜åŒ–ï¼Œè®¡æ•°: {no_change_count}")
                else:
                    no_change_count = 0
                    last_count = current_count

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰åŠ è½½æ›´å¤šå…ƒç´ 
                try:
                    loading_element = self.driver.find_element(By.CSS_SELECTOR, SELECTORS['loading_element'])
                    if loading_element.is_displayed():
                        logging.info("æ£€æµ‹åˆ°åŠ è½½æ›´å¤šå…ƒç´ ï¼Œç»§ç»­ç­‰å¾…...")
                        time.sleep(2)
                        continue
                except NoSuchElementException:
                    pass

                # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åº•éƒ¨
                try:
                    no_more_element = self.driver.find_element(By.CSS_SELECTOR, SELECTORS['no_more_element'])
                    if no_more_element.is_displayed():
                        logging.info("æ£€æµ‹åˆ°å·²åŠ è½½å…¨éƒ¨æ–‡ç« ")
                        break
                except NoSuchElementException:
                    pass

                time.sleep(1)

            final_count = len(self.driver.find_elements(By.CSS_SELECTOR, SELECTORS['album_items']))
            logging.info(f"æ–‡ç« åŠ è½½å®Œæˆï¼Œå…± {final_count} ç¯‡")
            return final_count

        except Exception as e:
            logging.error(f"åŠ è½½æ‰€æœ‰æ–‡ç« å¤±è´¥: {e}")
            return 0

    def extract_articles_list(self):
        """æå–æ–‡ç« åˆ—è¡¨ï¼Œæ”¯æŒURLå»é‡"""
        try:
            new_articles = []

            # è·å–æ‰€æœ‰æ–‡ç« å…ƒç´ 
            articles_elements = self.driver.find_elements(By.CSS_SELECTOR, SELECTORS['album_items'])
            logging.info(f"æ‰¾åˆ° {len(articles_elements)} ä¸ªæ–‡ç« å…ƒç´ ")

            for index, element in enumerate(articles_elements):
                try:
                    # æå–æ–‡ç« é“¾æ¥
                    article_url = element.get_attribute(SELECTORS['article_link'])
                    if not article_url:
                        continue

                    # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²ç»å­˜åœ¨ï¼ˆåŸºäºURLå»é‡ï¼‰
                    if self.articles_data:
                        exists, existing_article = check_article_exists_by_url(self.articles_data, article_url)
                        if exists:
                            logging.info(f"æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡: {article_url[:50]}...")
                            continue

                    # æå–æ–‡ç« åºå·
                    article_index = element.get_attribute('data-idx')
                    if article_index:
                        article_index = int(article_index)
                    else:
                        article_index = index + 1

                    # æå–é¢„è§ˆå†…å®¹
                    try:
                        preview_element = element.find_element(By.CSS_SELECTOR, SELECTORS['article_title'])
                        preview_text = preview_element.text.strip()
                    except NoSuchElementException:
                        preview_text = ""

                    # ä»é¢„è§ˆå†…å®¹æå–æ ‡é¢˜
                    title = extract_title_from_preview(preview_text)

                    article_info = {
                        'index': article_index,
                        'title': title,
                        'url': article_url,
                        'preview': preview_text[:100] + "..." if len(preview_text) > 100 else preview_text,
                        'status': 'pending',
                        'file_path': None,
                        'error_message': None,
                        'processed_time': None,
                        'retry_count': 0
                    }

                    new_articles.append(article_info)

                except Exception as e:
                    logging.error(f"æå–ç¬¬{index+1}ä¸ªæ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
                    continue

            # å¦‚æœæœ‰ç°æœ‰æ•°æ®ï¼Œè¿›è¡Œåˆå¹¶
            if self.articles_data and self.articles_data.get('articles'):
                logging.info(f"åˆå¹¶ {len(new_articles)} ç¯‡æ–°æ–‡ç« åˆ°ç°æœ‰æ•°æ®")
                self.articles_data = update_articles_with_url_matching(self.articles_data, new_articles)
                articles = self.articles_data['articles']
            else:
                articles = new_articles

            # æŒ‰åºå·æ’åº
            articles.sort(key=lambda x: x['index'])

            # åªè¿”å›å¾…å¤„ç†çš„æ–‡ç« 
            pending_articles = [a for a in articles if a['status'] == 'pending']
            logging.info(f"æˆåŠŸæå– {len(articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {len(pending_articles)} ç¯‡å¾…å¤„ç†")
            return articles

        except Exception as e:
            logging.error(f"æå–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def extract_article_content(self, article_url):
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹å’Œå‘å¸ƒæ—¶é—´"""
        try:
            logging.info(f"å¼€å§‹æå–æ–‡ç« å†…å®¹: {article_url}")

            # æ‰“å¼€æ–°æ ‡ç­¾é¡µ
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])

            # è®¿é—®æ–‡ç« é¡µé¢
            self.driver.get(article_url)

            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)

            # æå–å‘å¸ƒæ—¶é—´
            publish_time = extract_publish_time_from_article(self.driver)
            logging.info(f"æå–åˆ°å‘å¸ƒæ—¶é—´: {publish_time}")

            # æå–æ–‡ç« å†…å®¹
            content = ""
            try:
                # å°è¯•ä¸åŒçš„å†…å®¹é€‰æ‹©å™¨
                content_selectors = [
                    '#js_content',
                    '.rich_media_content',
                    '.content',
                    '#content'
                ]

                content_element = None
                for selector in content_selectors:
                    try:
                        content_element = WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        break
                    except TimeoutException:
                        continue

                if content_element:
                    # è·å–çº¯æ–‡æœ¬å†…å®¹
                    content = content_element.text.strip()
                    # æ¸…ç†å†…å®¹ï¼šå»é™¤ä»"æ”¶å½•äº"å¼€å§‹çš„éƒ¨åˆ†
                    content = self.clean_content(content)
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å†…å®¹å…ƒç´ ï¼Œå°è¯•è·å–æ•´ä¸ªé¡µé¢æ–‡æœ¬
                    content = self.driver.find_element(By.TAG_NAME, 'body').text.strip()
                    # æ¸…ç†å†…å®¹ï¼šå»é™¤ä»"æ”¶å½•äº"å¼€å§‹çš„éƒ¨åˆ†
                    content = self.clean_content(content)

            except Exception as e:
                logging.error(f"æå–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šè·å–é¡µé¢æ–‡æœ¬
                content = self.driver.find_element(By.TAG_NAME, 'body').text.strip()
                # æ¸…ç†å†…å®¹ï¼šå»é™¤ä»"æ”¶å½•äº"å¼€å§‹çš„éƒ¨åˆ†
                content = self.clean_content(content)

            # å…³é—­å½“å‰æ ‡ç­¾é¡µï¼Œè¿”å›ä¸»é¡µ
            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])

            logging.info(f"æ–‡ç« å†…å®¹æå–æˆåŠŸï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return content, publish_time

        except Exception as e:
            logging.error(f"æå–æ–‡ç« å†…å®¹å¼‚å¸¸: {e}")
            # ç¡®ä¿è¿”å›ä¸»é¡µ
            try:
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
            return None, None

    def _check_and_append_new_articles(self, album_url):
        """
        æ£€æŸ¥å¹¶è¿½åŠ æ–°æ–‡ç« åˆ°ç°æœ‰JSONæ–‡ä»¶ä¸­
        æ¯æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨æ‰§è¡Œæ­¤åŠŸèƒ½
        """
        try:
            logging.info("å¼€å§‹æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ç« ...")

            # ä¸´æ—¶è®¾ç½®é©±åŠ¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
            temp_driver = None
            if not self.driver:
                temp_driver = setup_driver(headless=self.headless, window_size=WINDOW_SIZE)
                self.driver = temp_driver

            # åŠ è½½ä¸“è¾‘é¡µé¢
            if not self.load_album_page(album_url):
                logging.warning("æ— æ³•åŠ è½½ä¸“è¾‘é¡µé¢è¿›è¡Œæ–°æ–‡ç« æ£€æµ‹")
                return

            # åŠ è½½æ‰€æœ‰æ–‡ç« 
            logging.info("æ­£åœ¨åŠ è½½ä¸“è¾‘é¡µé¢ä»¥æ£€æµ‹æ–°æ–‡ç« ...")
            loaded_count = self.load_all_articles()
            logging.info(f"é¡µé¢åŠ è½½å®Œæˆï¼Œå…±æ‰¾åˆ° {loaded_count} ç¯‡æ–‡ç« ")

            # æå–å½“å‰é¡µé¢çš„æ–‡ç« åˆ—è¡¨
            temp_articles_data = self.articles_data.copy() if self.articles_data else {'articles': []}
            temp_articles_data['articles'] = []  # æ¸…ç©ºæ–‡ç« åˆ—è¡¨ï¼Œé‡æ–°æå–
            self.articles_data = temp_articles_data

            articles = self.extract_articles_list()
            if not articles:
                logging.info("æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œæ— æ³•è¿›è¡Œæ–°æ–‡ç« æ£€æµ‹")
                return

            # é‡æ–°åŠ è½½åŸå§‹æ•°æ®
            original_data = load_json_state(JSON_FILE)
            if not original_data:
                logging.info("æœªæ‰¾åˆ°åŸå§‹æ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ–°æ–‡ç« æ£€æµ‹")
                return

            # æ”¶é›†ç°æœ‰æ–‡ç« çš„URLå“ˆå¸Œ
            existing_urls = set()
            if original_data.get('articles'):
                for article in original_data['articles']:
                    if article.get('url'):
                        existing_urls.add(extract_url_hash(article['url']))

            # æŸ¥æ‰¾æ–°æ–‡ç« 
            new_articles = []
            max_existing_index = 0

            # è·å–ç°æœ‰æœ€å¤§ç´¢å¼•
            if original_data.get('articles'):
                for article in original_data['articles']:
                    if article.get('index', 0) > max_existing_index:
                        max_existing_index = article['index']

            for article in articles:
                article_url_hash = extract_url_hash(article.get('url', ''))
                if article_url_hash not in existing_urls:
                    # ä¸ºæ–°æ–‡ç« åˆ†é…è¿ç»­çš„ç´¢å¼•å·
                    new_article = article.copy()
                    new_article['index'] = max_existing_index + len(new_articles) + 1
                    new_article['status'] = 'pending'
                    new_article['file_path'] = None
                    new_article['error_message'] = None
                    new_article['processed_time'] = None
                    new_article['retry_count'] = 0
                    new_articles.append(new_article)
                    logging.info(f"å‘ç°æ–°æ–‡ç« : {article.get('title', 'æœªçŸ¥æ ‡é¢˜')[:50]}...")

            if new_articles:
                logging.info(f"å‘ç° {len(new_articles)} ç¯‡æ–°æ–‡ç« ï¼Œå¼€å§‹è¿½åŠ åˆ°JSONæ–‡ä»¶...")

                # è¿½åŠ æ–°æ–‡ç« åˆ°åŸå§‹æ•°æ®
                original_data['articles'].extend(new_articles)

                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                original_data['total_articles'] = len(original_data['articles'])
                original_data['pending_count'] = len([a for a in original_data['articles'] if a['status'] == 'pending'])
                original_data['processed_count'] = len([a for a in original_data['articles'] if a['status'] == 'completed'])
                original_data['failed_count'] = len([a for a in original_data['articles'] if a['status'] == 'failed'])
                original_data['crawl_time'] = datetime.now().isoformat()

                # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                save_json_state(original_data, JSON_FILE)

                # æ›´æ–°å½“å‰å®ä¾‹çš„æ•°æ®
                self.articles_data = original_data

                logging.info(f"æˆåŠŸè¿½åŠ  {len(new_articles)} ç¯‡æ–°æ–‡ç« åˆ° wechat_articles.json")
                logging.info(f"æ›´æ–°åæ€»æ–‡ç« æ•°: {original_data['total_articles']}")
                logging.info(f"å¾…å¤„ç†æ–‡ç« æ•°: {original_data['pending_count']}")

                print(f"ğŸ” å‘ç° {len(new_articles)} ç¯‡æ–°æ–‡ç« å·²è‡ªåŠ¨è¿½åŠ åˆ°JSONæ–‡ä»¶")
                print(f"ğŸ“Š å½“å‰æ€»æ–‡ç« æ•°: {original_data['total_articles']}")
                print(f"â³ å¾…å¤„ç†æ–‡ç« æ•°: {original_data['pending_count']}")
            else:
                logging.info("æœªå‘ç°æ–°æ–‡ç« ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰æ•°æ®")
                print("âœ… æœªå‘ç°æ–°æ–‡ç« ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰æ•°æ®")

            # æ¸…ç†ä¸´æ—¶é©±åŠ¨
            if temp_driver:
                try:
                    temp_driver.quit()
                except:
                    pass
                self.driver = None

        except Exception as e:
            logging.error(f"æ£€æµ‹æ–°æ–‡ç« æ—¶å‡ºé”™: {e}")
            print(f"âš ï¸ æ£€æµ‹æ–°æ–‡ç« æ—¶å‡ºé”™ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰æ•°æ®: {e}")

            # ç¡®ä¿æ¸…ç†ä¸´æ—¶é©±åŠ¨
            if 'temp_driver' in locals() and temp_driver:
                try:
                    temp_driver.quit()
                except:
                    pass
                self.driver = None

    def clean_content(self, content):
        """
        æ¸…ç†æ–‡ç« å†…å®¹ï¼Œå»é™¤ä»"æ”¶å½•äº"å¼€å§‹çš„éƒ¨åˆ†

        Args:
            content (str): åŸå§‹æ–‡ç« å†…å®¹

        Returns:
            str: æ¸…ç†åçš„æ–‡ç« å†…å®¹
        """
        import re

        if not content:
            return content

        # æŸ¥æ‰¾"æ”¶å½•äº"çš„ä½ç½®
        pattern = r'\n?æ”¶å½•äº'
        match = re.search(pattern, content)

        if match:
            # ä¿ç•™"æ”¶å½•äº"ä¹‹å‰çš„å†…å®¹
            cleaned_content = content[:match.start()].rstrip()
            return cleaned_content
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°"æ”¶å½•äº"ï¼Œè¿”å›åŸå†…å®¹
            return content

    def process_article(self, article_info, output_dir):
        """å¤„ç†å•ä¸ªæ–‡ç« ï¼Œæ”¯æŒæ™ºèƒ½æ ‡é¢˜æå–å’Œå»é‡"""
        index = article_info['index']
        title = article_info['title']
        url = article_info['url']

        try:
            logging.info(f"å¼€å§‹å¤„ç†ç¬¬ {index} ç¯‡æ–‡ç« : {title}")

            # æå–æ–‡ç« å†…å®¹å’Œå‘å¸ƒæ—¶é—´
            content, publish_time = self.extract_article_content(url)
            if not content:
                raise Exception("æ–‡ç« å†…å®¹ä¸ºç©º")

            # å°è¯•ä»å†…å®¹ä¸­æå–çœŸå®æ ‡é¢˜
            real_title = extract_real_title_from_content(content)
            final_title = real_title if real_title else title

            # ä½¿ç”¨æ™ºèƒ½ä¿å­˜åŠŸèƒ½ï¼ˆä¼ å…¥å‘å¸ƒæ—¶é—´å’Œè®¡æ•°å™¨ï¼‰
            album_title = self.articles_data.get('album_title') if self.articles_data else None
            file_path, updated_counter = smart_save_article_content(url, final_title, content, output_dir, album_title, index, publish_time, getattr(self, 'date_counter', {}))
            if not file_path:
                raise Exception("ä¿å­˜æ–‡ç« å¤±è´¥")

            # æ›´æ–°è®¡æ•°å™¨
            if updated_counter:
                self.date_counter.update(updated_counter)
                self._last_updated_counter = updated_counter

            # æ›´æ–°æ–‡ç« ä¿¡æ¯ä¸­çš„çœŸå®æ ‡é¢˜
            article_info['title'] = final_title

            # æ‰¾åˆ°å¯¹åº”çš„æ–‡ç« ç´¢å¼•å¹¶æ›´æ–°çŠ¶æ€
            article_index = None
            for i, art in enumerate(self.articles_data['articles']):
                if art.get('url') == url or extract_url_hash(art.get('url', '')) == extract_url_hash(url):
                    article_index = i
                    break

            if article_index is not None:
                update_article_status(self.articles_data, article_index, status='completed', file_path=file_path)
            else:
                logging.warning(f"æ— æ³•æ‰¾åˆ°æ–‡ç« ç´¢å¼•: {url}")

            logging.info(f"ç¬¬ {index} ç¯‡æ–‡ç« å¤„ç†å®Œæˆ: {final_title}")
            return True

        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {e}"
            logging.error(f"ç¬¬ {index} ç¯‡æ–‡ç« å¤„ç†å¤±è´¥: {title}, {error_msg}")

            # æ‰¾åˆ°å¯¹åº”çš„æ–‡ç« ç´¢å¼•å¹¶æ›´æ–°çŠ¶æ€
            article_index = None
            for i, art in enumerate(self.articles_data['articles']):
                if art.get('url') == url or extract_url_hash(art.get('url', '')) == extract_url_hash(url):
                    article_index = i
                    break

            if article_index is not None:
                update_article_status(self.articles_data, article_index, status='failed', error_message=error_msg)
            else:
                logging.warning(f"æ— æ³•æ‰¾åˆ°æ–‡ç« ç´¢å¼•: {url}")

            return False

    def crawl_album(self, album_url, output_dir=ARTICLES_DIR, resume=True, retry_failed_only=False):
        """æŠ“å–ä¸“è¾‘æ–‡ç« """
        try:
            # åŠ è½½ç°æœ‰çŠ¶æ€
            if resume and os.path.exists(JSON_FILE):
                self.articles_data = load_json_state(JSON_FILE)
                if self.articles_data:
                    logging.info("åŠ è½½ç°æœ‰çŠ¶æ€æˆåŠŸ")
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ç« éœ€è¦è¿½åŠ 
                    if not retry_failed_only:
                        self._check_and_append_new_articles(album_url)
                else:
                    logging.info("åˆ›å»ºæ–°çš„çŠ¶æ€æ–‡ä»¶")
                    self.articles_data = None

            # è®¾ç½®è¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)

            # åŠ è½½æ—¥æœŸè®¡æ•°å™¨
            album_title = self.articles_data.get('album_title') if self.articles_data else None
            self.date_counter = load_date_counter(output_dir, album_title) if album_title else {}

            # å¦‚æœåªé‡è¯•å¤±è´¥çš„æ–‡ç« ï¼Œè¿‡æ»¤æ–‡ç« åˆ—è¡¨
            if retry_failed_only and self.articles_data:
                failed_articles = [a for a in self.articles_data['articles'] if a['status'] == 'failed']
                if not failed_articles:
                    logging.info("æ²¡æœ‰å¤±è´¥çš„æ–‡ç« éœ€è¦é‡è¯•")
                    return True

                # é‡ç½®å¤±è´¥æ–‡ç« çŠ¶æ€ä¸ºpending
                for article in failed_articles:
                    article['status'] = 'pending'
                    article['error_message'] = None
                    article['retry_count'] += 1

                logging.info(f"é‡è¯• {len(failed_articles)} ç¯‡å¤±è´¥çš„æ–‡ç« ")

            # å¦‚æœæ²¡æœ‰ç°æœ‰æ•°æ®æˆ–ä¸éœ€è¦æ¢å¤ï¼Œé‡æ–°æŠ“å–
            if not self.articles_data or not resume:
                # è®¾ç½®é©±åŠ¨
                if not self.setup_driver():
                    return False

                # åŠ è½½ä¸“è¾‘é¡µé¢
                if not self.load_album_page(album_url):
                    return False

                # æå–ä¸“è¾‘ä¿¡æ¯
                album_title, total_articles = self.extract_album_info()

                # åŠ è½½æ‰€æœ‰æ–‡ç« 
                loaded_count = self.load_all_articles()

                # æå–æ–‡ç« åˆ—è¡¨
                articles = self.extract_articles_list()

                # åˆå§‹åŒ–æ•°æ®ç»“æ„
                self.articles_data = {
                    'album_title': album_title,
                    'album_url': album_url,
                    'total_articles': len(articles),
                    'processed_count': 0,
                    'failed_count': 0,
                    'pending_count': len(articles),
                    'crawl_time': datetime.now().isoformat(),
                    'articles': articles
                }

                # ä¿å­˜åˆå§‹çŠ¶æ€
                save_json_state(self.articles_data, JSON_FILE)

            # è®¾ç½®é©±åŠ¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
            if not self.driver:
                if not self.setup_driver():
                    return False

            # ç¡®ä¿åœ¨ä¸“è¾‘é¡µé¢
            if album_url not in self.driver.current_url:
                self.load_album_page(album_url)

            # å¤„ç†å¾…å¤„ç†çš„æ–‡ç« 
            pending_articles = [a for a in self.articles_data['articles'] if a['status'] == 'pending']

            if not pending_articles:
                logging.info("æ²¡æœ‰å¾…å¤„ç†çš„æ–‡ç« ")
                return True

            logging.info(f"å¼€å§‹å¤„ç† {len(pending_articles)} ç¯‡å¾…å¤„ç†æ–‡ç« ")

            success_count = 0
            for i, article_info in enumerate(pending_articles):
                # æ˜¾ç¤ºè¿›åº¦
                progress = format_progress_bar(
                    i + 1, len(pending_articles),
                    prefix=f"å¤„ç†è¿›åº¦",
                    suffix=f"{i + 1}/{len(pending_articles)}"
                )
                print(f"\r{progress}", end="", flush=True)

                # å¤„ç†æ–‡ç« 
                if self.process_article(article_info, output_dir):
                    success_count += 1

                # ä¿å­˜çŠ¶æ€å’Œæ—¥æœŸè®¡æ•°å™¨
                save_json_state(self.articles_data, JSON_FILE)
                if album_title:
                    save_date_counter(output_dir, album_title, self.date_counter)

                # æ›´æ–°è®¡æ•°å™¨ï¼ˆä»ä¿å­˜å‡½æ•°è·å–çš„æ›´æ–°ï¼‰
                if hasattr(self, '_last_updated_counter'):
                    self.date_counter.update(self._last_updated_counter)

                # å»¶æ—¶
                if i < len(pending_articles) - 1:  # ä¸æ˜¯æœ€åä¸€ç¯‡
                    delay = get_random_delay()
                    logging.info(f"ç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)

            print()  # æ¢è¡Œ

            # æœ€ç»ˆç»Ÿè®¡
            final_completed = sum(1 for a in self.articles_data['articles'] if a['status'] == 'completed')
            final_failed = sum(1 for a in self.articles_data['articles'] if a['status'] == 'failed')

            logging.info(f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {final_completed}, å¤±è´¥: {final_failed}")
            print(f"\nå¤„ç†å®Œæˆï¼")
            print(f"æ€»æ–‡ç« æ•°: {len(self.articles_data['articles'])}")
            print(f"æˆåŠŸå¤„ç†: {final_completed}")
            print(f"å¤„ç†å¤±è´¥: {final_failed}")
            print(f"æ–‡ç« ä¿å­˜åœ¨: {output_dir}")

            return final_completed > 0

        except Exception as e:
            logging.error(f"æŠ“å–ä¸“è¾‘å¤±è´¥: {e}")
            return False

        finally:
            # å…³é—­æµè§ˆå™¨
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¾®ä¿¡å…¬ä¼—å·ä¸“è¾‘æ–‡ç« æŠ“å–å·¥å…·')
    parser.add_argument('--url', required=True, help='å¾®ä¿¡å…¬ä¼—å·ä¸“è¾‘é“¾æ¥')
    parser.add_argument('--output', default=ARTICLES_DIR, help='æ–‡ç« ä¿å­˜ç›®å½•')
    parser.add_argument('--delay', type=int, default=DEFAULT_DELAY, help='è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--no-resume', action='store_false', dest='resume', help='ä¸ä»æ–­ç‚¹ç»§ç»­ï¼Œé‡æ–°å¼€å§‹')
    parser.add_argument('--retry-failed', action='store_true', help='ä»…é‡è¯•å¤±è´¥çš„æ–‡ç« ')
    parser.add_argument('--headless', action='store_true', help='æ— å¤´æ¨¡å¼è¿è¡Œ')

    args = parser.parse_args()

    # éªŒè¯URL
    if not validate_url(args.url):
        print("é”™è¯¯ï¼šæ— æ•ˆçš„URL")
        return 1

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(args.output, exist_ok=True)
    os.makedirs(LOGS_DIR, exist_ok=True)

    # åˆ›å»ºæŠ“å–å™¨
    crawler = WeChatAlbumCrawler(headless=args.headless, delay=args.delay)

    # å¼€å§‹æŠ“å–
    try:
        success = crawler.crawl_album(
            album_url=args.url,
            output_dir=args.output,
            resume=args.resume,
            retry_failed_only=args.retry_failed
        )
        return 0 if success else 1

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        logging.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1

    except Exception as e:
        print(f"\nç¨‹åºå¼‚å¸¸: {e}")
        logging.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())